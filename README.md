# Adacomp
A Zeroth-order Adaptive Learning Rate Method to Reduce Cost of Hyperparameters Tuning for Deep Learning

This provides the code, data, and experiments for article "A Zeroth-order Adaptive Learning Rate Method to Reduce Cost of Hyperparameters Tuning for Deep Learning", which has been submitted to journal Applied Sciences. The method, named Adacomp, adaptively adjusts the learning rate only based on values of loss function. From high abstract, Adacomp penalizes learning rate when loss value decreses and compensates learning rate in the contrast. 

# Aim
Anyone who is interested in Adacomp can reproduce the experimental results, or makes a further study based on the provided code. 

# Structure
1. Code for MNIST (10 classification)
2. Code for KMNIST (10 classification)
3. Code for Fashion-MNIST (10 classification)
4. Code for CIFAR-10 (10 classification)
5. Code for CIFAR-100 (100 classification)
